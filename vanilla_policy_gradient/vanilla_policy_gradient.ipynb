{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_obs_params = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.n\n",
    "\n",
    "n_obs_params, n_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(n_obs_params, )),\n",
    "    tf.keras.layers.Dense(n_acts, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the output of our network given an observation from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.50605947 0.4939405 ], shape=(2,), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_obs = env.reset()\n",
    "sample_obs = np.expand_dims(sample_obs, axis=0)\n",
    "sample_obs = np.asarray(sample_obs, dtype=np.float32)\n",
    "\n",
    "prob_logits = net(sample_obs)[0]\n",
    "action_chosen = tf.argmax(prob_logits)\n",
    "\n",
    "print(prob_logits)\n",
    "print(action_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(net, render=False):\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if render: env.render()\n",
    "        \n",
    "        observations.append(obs)\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        obs = np.asarray(obs, dtype=np.float32)\n",
    "        logits = net(obs)\n",
    "        act = tf.squeeze(tf.multinomial(logits=logits, num_samples=1), axis=1)[0].numpy()\n",
    "        actions.append(act)\n",
    "        next_obs, reward, done, info = env.step(act)\n",
    "        rewards.append(reward)\n",
    "        obs = next_obs\n",
    "        \n",
    "    return observations, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-f5bb32fa1660>:17: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([ 0.04902787, -0.01344601, -0.00413251, -0.00987001]),\n",
       "  array([ 0.04875895, -0.20850845, -0.00432991,  0.28150621]),\n",
       "  array([ 0.04458878, -0.013325  ,  0.00130022, -0.01253921]),\n",
       "  array([ 0.04432228, -0.20846558,  0.00104943,  0.28055367]),\n",
       "  array([ 0.04015296, -0.01335861,  0.00666051, -0.01179808]),\n",
       "  array([ 0.03988579, -0.20857545,  0.00642454,  0.28297885]),\n",
       "  array([ 0.03571428, -0.01354572,  0.01208412, -0.0076709 ]),\n",
       "  array([ 0.03544337, -0.20883887,  0.0119307 ,  0.28880008]),\n",
       "  array([ 3.12665919e-02, -1.38890649e-02,  1.77067053e-02, -9.62716415e-05]),\n",
       "  array([ 0.03098881,  0.18097453,  0.01770478, -0.28714034]),\n",
       "  array([ 0.0346083 ,  0.37583958,  0.01196197, -0.57418717]),\n",
       "  array([ 0.04212509,  0.18055198,  0.00047823, -0.27776   ]),\n",
       "  array([ 0.04573613, -0.01457679, -0.00507697,  0.01507372]),\n",
       "  array([ 0.0454446 , -0.20962556, -0.0047755 ,  0.30615049]),\n",
       "  array([ 0.04125209, -0.40467914,  0.00134751,  0.59732353]),\n",
       "  array([ 0.0331585 , -0.20957607,  0.01329398,  0.30506536]),\n",
       "  array([ 0.02896698, -0.40488492,  0.01939529,  0.60191107]),\n",
       "  array([ 0.02086928, -0.21003957,  0.03143351,  0.31539986]),\n",
       "  array([ 0.01666849, -0.40559484,  0.03774151,  0.6178278 ]),\n",
       "  array([ 0.00855659, -0.60122311,  0.05009807,  0.92215461]),\n",
       "  array([-0.00346787, -0.40681257,  0.06854116,  0.64562734]),\n",
       "  array([-0.01160412, -0.21270929,  0.0814537 ,  0.37529138]),\n",
       "  array([-0.0158583 , -0.01883307,  0.08895953,  0.10936218]),\n",
       "  array([-0.01623497,  0.17490893,  0.09114678, -0.15398204]),\n",
       "  array([-0.01273679,  0.36861561,  0.08806714, -0.41657589]),\n",
       "  array([-0.00536448,  0.56238628,  0.07973562, -0.6802472 ]),\n",
       "  array([ 0.00588325,  0.75631548,  0.06613067, -0.94679933]),\n",
       "  array([ 0.02100956,  0.56036829,  0.04719469, -0.63409275]),\n",
       "  array([ 0.03221693,  0.36462089,  0.03451283, -0.3269285 ]),\n",
       "  array([ 0.03950934,  0.169025  ,  0.02797426, -0.02356463]),\n",
       "  array([ 0.04288984,  0.36373484,  0.02750297, -0.30729176]),\n",
       "  array([ 0.05016454,  0.55845431,  0.02135713, -0.59117566]),\n",
       "  array([ 0.06133363,  0.36303997,  0.00953362, -0.29184246]),\n",
       "  array([ 0.06859443,  0.5580247 ,  0.00369677, -0.58150342]),\n",
       "  array([ 0.07975492,  0.75309465, -0.0079333 , -0.87301951]),\n",
       "  array([ 0.09481681,  0.94832358, -0.02539369, -1.16818605]),\n",
       "  array([ 0.11378328,  1.14376653, -0.04875741, -1.4687209 ]),\n",
       "  array([ 0.13665862,  1.33945004, -0.07813183, -1.77622634]),\n",
       "  array([ 0.16344762,  1.53536039, -0.11365635, -2.09214274]),\n",
       "  array([ 0.19415482,  1.73142951, -0.15549921, -2.41769196]),\n",
       "  array([ 0.22878341,  1.53795823, -0.20385305, -2.17651933])],\n",
       " [0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_episode(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_rewards_to_advantages(rewards):\n",
    "    for i in range(-2, -len(rewards)-1, -1):\n",
    "        rewards[i] += rewards[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [1, 2, 3, 4]\n",
    "_convert_rewards_to_advantages(test_list)\n",
    "\n",
    "assert test_list == [10, 9, 7, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, observations, actions, advantages, learning_rate=1e-3):\n",
    "    \n",
    "    observations = np.array(observations, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    advantages = np.array(advantages, dtype=np.float32)\n",
    "    \n",
    "    assert len(observations) == len(actions) == len(advantages)\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        probs_actions = net(observations)\n",
    "        action_masks = tf.one_hot(actions, n_acts)\n",
    "        \n",
    "        log_pi = tf.reduce_sum(action_masks * tf.nn.log_softmax(probs_actions), axis=1)\n",
    "        \n",
    "        loss = -tf.reduce_mean( log_pi * advantages )\n",
    "    \n",
    "    d_loss_d_w = t.gradient(loss, net.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(d_loss_d_w, net.trainable_weights))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, actions, rewards = play_episode(net)\n",
    "_convert_rewards_to_advantages(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1520, shape=(), dtype=float32, numpy=10.734823>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(net, tf.train.AdamOptimizer(learning_rate=1e-2), observations, actions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24: Reward = 24.16\n",
      "Loss = 10.904436111450195\n",
      "Episode 49: Reward = 24.04\n",
      "Loss = 10.034758567810059\n",
      "Episode 74: Reward = 23.6\n",
      "Loss = 10.494437217712402\n",
      "Episode 99: Reward = 30.52\n",
      "Loss = 14.820843696594238\n",
      "Episode 124: Reward = 44.84\n",
      "Loss = 20.784473419189453\n",
      "Episode 149: Reward = 27.56\n",
      "Loss = 11.766745567321777\n",
      "Episode 174: Reward = 32.88\n",
      "Loss = 14.751398086547852\n",
      "Episode 199: Reward = 28.88\n",
      "Loss = 13.427520751953125\n",
      "Episode 224: Reward = 39.52\n",
      "Loss = 17.536441802978516\n",
      "Episode 249: Reward = 35.2\n",
      "Loss = 13.619742393493652\n",
      "Episode 274: Reward = 48.88\n",
      "Loss = 21.243408203125\n",
      "Episode 299: Reward = 38.68\n",
      "Loss = 16.6977481842041\n",
      "Episode 324: Reward = 44.56\n",
      "Loss = 18.12976837158203\n",
      "Episode 349: Reward = 42.84\n",
      "Loss = 18.616130828857422\n",
      "Episode 374: Reward = 55.2\n",
      "Loss = 23.183975219726562\n",
      "Episode 399: Reward = 46.36\n",
      "Loss = 16.62848663330078\n",
      "Episode 424: Reward = 51.28\n",
      "Loss = 20.087614059448242\n",
      "Episode 449: Reward = 44.84\n",
      "Loss = 18.27011489868164\n",
      "Episode 474: Reward = 66.64\n",
      "Loss = 26.13983154296875\n",
      "Episode 499: Reward = 74.48\n",
      "Loss = 28.674985885620117\n",
      "Episode 524: Reward = 63.12\n",
      "Loss = 22.923471450805664\n",
      "Episode 549: Reward = 78.12\n",
      "Loss = 33.09186553955078\n",
      "Episode 574: Reward = 63.36\n",
      "Loss = 23.232589721679688\n",
      "Episode 599: Reward = 66.8\n",
      "Loss = 26.81240463256836\n",
      "Episode 624: Reward = 70.92\n",
      "Loss = 27.533676147460938\n",
      "Episode 649: Reward = 87.76\n",
      "Loss = 33.395633697509766\n",
      "Episode 674: Reward = 112.72\n",
      "Loss = 41.10208511352539\n",
      "Episode 699: Reward = 102.32\n",
      "Loss = 36.53812026977539\n",
      "Episode 724: Reward = 90.52\n",
      "Loss = 34.1726188659668\n",
      "Episode 749: Reward = 107.2\n",
      "Loss = 38.34192657470703\n",
      "Episode 774: Reward = 112.64\n",
      "Loss = 36.680973052978516\n",
      "Episode 799: Reward = 127.32\n",
      "Loss = 42.641990661621094\n",
      "Episode 824: Reward = 127.4\n",
      "Loss = 40.056053161621094\n",
      "Episode 849: Reward = 129.16\n",
      "Loss = 43.238399505615234\n",
      "Episode 874: Reward = 140.68\n",
      "Loss = 44.37211608886719\n",
      "Episode 899: Reward = 139.2\n",
      "Loss = 44.26424026489258\n",
      "Episode 924: Reward = 138.52\n",
      "Loss = 42.33627700805664\n",
      "Episode 949: Reward = 149.08\n",
      "Loss = 45.100521087646484\n",
      "Episode 974: Reward = 157.8\n",
      "Loss = 47.47492980957031\n",
      "Episode 999: Reward = 154.08\n",
      "Loss = 47.178855895996094\n",
      "Episode 1024: Reward = 155.16\n",
      "Loss = 45.38117599487305\n",
      "Episode 1049: Reward = 170.8\n",
      "Loss = 49.336063385009766\n",
      "Episode 1074: Reward = 176.04\n",
      "Loss = 50.115074157714844\n",
      "Episode 1099: Reward = 173.16\n",
      "Loss = 49.048152923583984\n",
      "Episode 1124: Reward = 188.96\n",
      "Loss = 52.047969818115234\n",
      "Episode 1149: Reward = 197.0\n",
      "Loss = 52.60543441772461\n",
      "Episode 1174: Reward = 191.52\n",
      "Loss = 52.004188537597656\n",
      "Episode 1199: Reward = 190.16\n",
      "Loss = 52.02395248413086\n",
      "Episode 1224: Reward = 185.92\n",
      "Loss = 50.72209548950195\n",
      "Episode 1249: Reward = 194.6\n",
      "Loss = 51.73423767089844\n",
      "Episode 1274: Reward = 184.24\n",
      "Loss = 48.89017105102539\n",
      "Episode 1299: Reward = 198.68\n",
      "Loss = 52.87312698364258\n",
      "Episode 1324: Reward = 196.8\n",
      "Loss = 52.16524887084961\n",
      "Episode 1349: Reward = 186.6\n",
      "Loss = 50.0870475769043\n",
      "Episode 1374: Reward = 195.48\n",
      "Loss = 53.010074615478516\n",
      "Episode 1399: Reward = 197.64\n",
      "Loss = 52.000335693359375\n",
      "Episode 1424: Reward = 191.2\n",
      "Loss = 50.99871063232422\n",
      "Episode 1449: Reward = 199.76\n",
      "Loss = 52.5456428527832\n",
      "Stopping Criteria Achieved\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "n_episodes_before_train = 25\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(n_obs_params, )),\n",
    "    tf.keras.layers.Dense(n_acts)\n",
    "])\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    ep_observations, ep_actions, ep_rewards = play_episode(net)\n",
    "    observations += ep_observations\n",
    "    actions += ep_actions\n",
    "    ep_reward = sum(ep_rewards)\n",
    "    episode_rewards.append(ep_reward)\n",
    "    _convert_rewards_to_advantages(ep_rewards)\n",
    "    rewards += ep_rewards\n",
    "    \n",
    "    if (episode + 1) % n_episodes_before_train == 0:\n",
    "        loss = train(net, optimizer, observations, actions, rewards)\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        print('Episode {}: Reward = {}'.format(episode, np.mean(episode_rewards[-n_episodes_before_train:])))\n",
    "        print('Loss = {}'.format(loss))\n",
    "        \n",
    "        if len(episode_rewards) > 100 and np.mean(episode_rewards[-100:]) >= 195:\n",
    "            net.save_weights('net.h5')\n",
    "            print('Stopping Criteria Achieved')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are to check why the loss increases while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_check_loss(net, optimizer, observations, actions, advantages, sub_observations, sub_actions,\n",
    "                     sub_advantages, learning_rate=1e-3):\n",
    "    observations = np.array(observations, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    advantages = np.array(advantages, dtype=np.float32)\n",
    "    \n",
    "    sub_observations = np.array(sub_observations, dtype=np.float32)\n",
    "    sub_actions = np.array(sub_actions, dtype=np.int32)\n",
    "    sub_advantages = np.array(sub_advantages, dtype=np.float32)\n",
    "    \n",
    "    assert len(observations) == len(actions) == len(advantages)\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        probs_actions = net(observations)\n",
    "        action_masks = tf.one_hot(actions, n_acts)\n",
    "        \n",
    "        sub_probs_actions = net(sub_observations)\n",
    "        sub_action_masks = tf.one_hot(sub_actions, n_acts)\n",
    "        \n",
    "        log_pi_sub = tf.reduce_sum(sub_action_masks * tf.nn.log_softmax(sub_probs_actions), axis=1)\n",
    "        sub_loss = -tf.reduce_mean( log_pi_sub * sub_advantages )\n",
    "        \n",
    "        print('sub_loss:', sub_loss.numpy())\n",
    "        \n",
    "        log_pi = tf.reduce_sum(action_masks * tf.nn.log_softmax(probs_actions), axis=1)\n",
    "        loss = -tf.reduce_mean( log_pi * advantages )\n",
    "    \n",
    "    d_loss_d_w = t.gradient(loss, net.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(d_loss_d_w, net.trainable_weights))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_loss: 8.132531\n",
      "Episode 24: Reward = 16.16\n",
      "Loss = 6.4081268310546875\n",
      "sub_loss: 9.2115555\n",
      "Episode 49: Reward = 19.28\n",
      "Loss = 8.252080917358398\n",
      "sub_loss: 10.024085\n",
      "Episode 74: Reward = 21.32\n",
      "Loss = 9.238167762756348\n",
      "sub_loss: 10.743457\n",
      "Episode 99: Reward = 23.36\n",
      "Loss = 10.934904098510742\n",
      "sub_loss: 10.735608\n",
      "Episode 124: Reward = 20.12\n",
      "Loss = 8.406343460083008\n",
      "sub_loss: 10.977724\n",
      "Episode 149: Reward = 22.52\n",
      "Loss = 10.14055347442627\n",
      "sub_loss: 11.53183\n",
      "Episode 174: Reward = 26.52\n",
      "Loss = 11.65504264831543\n",
      "sub_loss: 12.560639\n",
      "Episode 199: Reward = 34.12\n",
      "Loss = 18.131317138671875\n",
      "sub_loss: 12.978116\n",
      "Episode 224: Reward = 28.76\n",
      "Loss = 11.513986587524414\n",
      "sub_loss: 13.300332\n",
      "Episode 249: Reward = 28.68\n",
      "Loss = 11.460155487060547\n",
      "sub_loss: 13.863955\n",
      "Episode 274: Reward = 33.68\n",
      "Loss = 15.206086158752441\n",
      "sub_loss: 14.5930605\n",
      "Episode 299: Reward = 38.92\n",
      "Loss = 16.90174674987793\n",
      "sub_loss: 15.300336\n",
      "Episode 324: Reward = 40.8\n",
      "Loss = 16.90399742126465\n",
      "sub_loss: 16.158827\n",
      "Episode 349: Reward = 46.92\n",
      "Loss = 20.60100555419922\n",
      "sub_loss: 16.881102\n",
      "Episode 374: Reward = 48.16\n",
      "Loss = 19.76093864440918\n",
      "sub_loss: 17.488794\n",
      "Episode 399: Reward = 47.96\n",
      "Loss = 19.496278762817383\n",
      "sub_loss: 18.110563\n",
      "Episode 424: Reward = 46.64\n",
      "Loss = 17.340038299560547\n",
      "sub_loss: 18.680319\n",
      "Episode 449: Reward = 47.68\n",
      "Loss = 18.03484344482422\n",
      "sub_loss: 19.19774\n",
      "Episode 474: Reward = 50.88\n",
      "Loss = 18.501811981201172\n",
      "sub_loss: 19.784687\n",
      "Episode 499: Reward = 52.76\n",
      "Loss = 18.362422943115234\n",
      "sub_loss: 20.422508\n",
      "Episode 524: Reward = 54.92\n",
      "Loss = 21.601360321044922\n",
      "sub_loss: 21.025688\n",
      "Episode 549: Reward = 59.24\n",
      "Loss = 20.156017303466797\n",
      "sub_loss: 21.41216\n",
      "Episode 574: Reward = 51.72\n",
      "Loss = 18.111650466918945\n",
      "sub_loss: 21.857225\n",
      "Episode 599: Reward = 57.76\n",
      "Loss = 19.925556182861328\n",
      "sub_loss: 22.207067\n",
      "Episode 624: Reward = 53.44\n",
      "Loss = 17.250932693481445\n",
      "sub_loss: 22.81656\n",
      "Episode 649: Reward = 66.36\n",
      "Loss = 23.028491973876953\n",
      "sub_loss: 23.308807\n",
      "Episode 674: Reward = 64.08\n",
      "Loss = 23.24660301208496\n",
      "sub_loss: 23.752907\n",
      "Episode 699: Reward = 61.24\n",
      "Loss = 21.478994369506836\n",
      "sub_loss: 24.206419\n",
      "Episode 724: Reward = 67.64\n",
      "Loss = 22.878822326660156\n",
      "sub_loss: 24.909346\n",
      "Episode 749: Reward = 78.44\n",
      "Loss = 26.374906539916992\n",
      "sub_loss: 25.529583\n",
      "Episode 774: Reward = 81.08\n",
      "Loss = 27.89712905883789\n",
      "sub_loss: 25.751278\n",
      "Episode 799: Reward = 55.52\n",
      "Loss = 18.561229705810547\n",
      "sub_loss: 26.388361\n",
      "Episode 824: Reward = 79.88\n",
      "Loss = 26.19620704650879\n",
      "sub_loss: 27.09809\n",
      "Episode 849: Reward = 84.2\n",
      "Loss = 27.26166343688965\n",
      "sub_loss: 28.059294\n",
      "Episode 874: Reward = 100.4\n",
      "Loss = 33.19108200073242\n",
      "sub_loss: 28.764723\n",
      "Episode 899: Reward = 86.88\n",
      "Loss = 29.674694061279297\n",
      "sub_loss: 29.603468\n",
      "Episode 924: Reward = 99.52\n",
      "Loss = 32.94485092163086\n",
      "sub_loss: 30.293756\n",
      "Episode 949: Reward = 87.76\n",
      "Loss = 30.41538429260254\n",
      "sub_loss: 30.760916\n",
      "Episode 974: Reward = 84.76\n",
      "Loss = 28.622562408447266\n",
      "sub_loss: 31.439089\n",
      "Episode 999: Reward = 101.84\n",
      "Loss = 33.21638488769531\n",
      "sub_loss: 32.087788\n",
      "Episode 1024: Reward = 105.44\n",
      "Loss = 35.77585220336914\n",
      "sub_loss: 32.64911\n",
      "Episode 1049: Reward = 105.04\n",
      "Loss = 35.28740692138672\n",
      "sub_loss: 33.62268\n",
      "Episode 1074: Reward = 154.8\n",
      "Loss = 45.62184524536133\n",
      "sub_loss: 34.34304\n",
      "Episode 1099: Reward = 135.08\n",
      "Loss = 40.696563720703125\n",
      "sub_loss: 35.376156\n",
      "Episode 1124: Reward = 153.12\n",
      "Loss = 44.682403564453125\n",
      "sub_loss: 36.47129\n",
      "Episode 1149: Reward = 153.32\n",
      "Loss = 44.37015151977539\n",
      "sub_loss: 37.702423\n",
      "Episode 1174: Reward = 159.88\n",
      "Loss = 45.89851760864258\n",
      "sub_loss: 38.60419\n",
      "Episode 1199: Reward = 156.16\n",
      "Loss = 44.783348083496094\n",
      "sub_loss: 39.770206\n",
      "Episode 1224: Reward = 168.8\n",
      "Loss = 47.40376663208008\n",
      "sub_loss: 40.75936\n",
      "Episode 1249: Reward = 170.52\n",
      "Loss = 48.266265869140625\n",
      "sub_loss: 41.67457\n",
      "Episode 1274: Reward = 176.52\n",
      "Loss = 49.014156341552734\n",
      "sub_loss: 42.606667\n",
      "Episode 1299: Reward = 179.12\n",
      "Loss = 49.76319885253906\n",
      "sub_loss: 43.606747\n",
      "Episode 1324: Reward = 183.36\n",
      "Loss = 50.645416259765625\n",
      "sub_loss: 44.517826\n",
      "Episode 1349: Reward = 171.8\n",
      "Loss = 49.13068771362305\n",
      "sub_loss: 45.546055\n",
      "Episode 1374: Reward = 192.36\n",
      "Loss = 51.71648025512695\n",
      "sub_loss: 46.45341\n",
      "Episode 1399: Reward = 176.72\n",
      "Loss = 48.60637664794922\n",
      "sub_loss: 47.406067\n",
      "Episode 1424: Reward = 196.08\n",
      "Loss = 53.1319694519043\n",
      "sub_loss: 48.295635\n",
      "Episode 1449: Reward = 183.0\n",
      "Loss = 50.704139709472656\n",
      "sub_loss: 49.393227\n",
      "Episode 1474: Reward = 190.92\n",
      "Loss = 53.1491813659668\n",
      "sub_loss: 50.41894\n",
      "Episode 1499: Reward = 195.84\n",
      "Loss = 54.08894348144531\n",
      "sub_loss: 51.31302\n",
      "Episode 1524: Reward = 184.64\n",
      "Loss = 50.6281623840332\n",
      "sub_loss: 52.26732\n",
      "Episode 1549: Reward = 195.64\n",
      "Loss = 52.9018440246582\n",
      "sub_loss: 53.32924\n",
      "Episode 1574: Reward = 195.52\n",
      "Loss = 52.82029342651367\n",
      "sub_loss: 54.36645\n",
      "Episode 1599: Reward = 199.92\n",
      "Loss = 54.441734313964844\n",
      "sub_loss: 55.37628\n",
      "Episode 1624: Reward = 193.6\n",
      "Loss = 52.8986701965332\n",
      "Stopping Criteria Achieved\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "n_episodes_before_train = 25\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "sub_observations = []\n",
    "sub_actions = []\n",
    "sub_rewards = []\n",
    "\n",
    "net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(n_obs_params, )),\n",
    "    tf.keras.layers.Dense(n_acts)\n",
    "])\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    ep_observations, ep_actions, ep_rewards = play_episode(net)\n",
    "    observations += ep_observations\n",
    "    actions += ep_actions\n",
    "    \n",
    "    ep_reward = sum(ep_rewards)\n",
    "    episode_rewards.append(ep_reward)\n",
    "    _convert_rewards_to_advantages(ep_rewards)\n",
    "    rewards += ep_rewards\n",
    "\n",
    "    sub_observations += ep_observations if len(ep_observations) < 10 else ep_observations[:10]\n",
    "    sub_actions += ep_actions if len(ep_actions) < 10 else ep_actions[:10]\n",
    "    sub_rewards += ep_rewards if len(ep_rewards) < 10 else ep_rewards[:10]\n",
    "    \n",
    "    if (episode + 1) % n_episodes_before_train == 0:\n",
    "        loss = train_check_loss(net, optimizer, observations, actions, rewards, sub_observations, sub_actions, sub_rewards)\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        print('Episode {}: Reward = {}'.format(episode, np.mean(episode_rewards[-n_episodes_before_train:])))\n",
    "        print('Loss = {}'.format(loss))\n",
    "        \n",
    "        if len(episode_rewards) > 100 and np.mean(episode_rewards[-100:]) >= 195:\n",
    "            net.save_weights('net.h5')\n",
    "            print('Stopping Criteria Achieved')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
