{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"two_armed_bandit.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"vK4R7HOQMCrL","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wGxVmCIBMMbu","colab_type":"code","colab":{}},"cell_type":"code","source":["num_machines = 3                               # Choice of machines from which we can choose to select anyone, all machines having a random reward, but with different probabilities of success\n","\n","reward_thresholds = np.linspace(start=-2, stop=2, num=num_machines).tolist()\n","\n","def get_reward(reward_threshold):              # This function computes reward which is a probabilistic function of reward threshold\n","  probability = np.random.normal(loc=0.0, scale=1.0, size=None)\n","  return 1 if probability < reward_threshold else -1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c4KyyYkFOWYs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"f0d9ed7e-6cf8-4ad4-855e-531ab3d5eeb3","executionInfo":{"status":"ok","timestamp":1535375067695,"user_tz":-330,"elapsed":944,"user":{"displayName":"Vaibhav Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101533172066540342731"}}},"cell_type":"code","source":["# Let's test out how well this function does\n","print('reward_thresholds:', reward_thresholds)\n","\n","for reward_threshold in np.random.uniform(0, 1, size=10):\n","  print( get_reward(reward_threshold) )"],"execution_count":3,"outputs":[{"output_type":"stream","text":["reward_thresholds: [-2.0, 0.0, 2.0]\n","1\n","-1\n","1\n","1\n","-1\n","1\n","1\n","1\n","1\n","1\n"],"name":"stdout"}]},{"metadata":{"id":"PVmwh4aMO32G","colab_type":"code","colab":{}},"cell_type":"code","source":["# Let's build out our model\n","\n","tf.reset_default_graph()\n","\n","weights = tf.Variable(tf.ones([num_machines]))\n","chosen_action = tf.argmax(weights, axis=0)\n","\n","reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n","action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n","responsible_weight = tf.slice(input_=weights, begin=action_holder, size=[1])\n","loss = -(tf.log(responsible_weight) * reward_holder)\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n","update_model = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6GH08bSTQexS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":884},"outputId":"4e68783c-14b5-4f2a-fa25-c2d25f6bd01b","executionInfo":{"status":"ok","timestamp":1535375704699,"user_tz":-330,"elapsed":2578,"user":{"displayName":"Vaibhav Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"101533172066540342731"}}},"cell_type":"code","source":["# Let's train our model\n","\n","num_episodes = 1000\n","total_reward = np.zeros(num_machines)\n","num_trials = np.zeros(num_machines)\n","\n","eps = 0.1\n","init = tf.initialize_all_variables()\n","\n","\n","def select_action(sess, eps):\n","  probability = random.uniform(0, 1)\n","  if probability < eps:\n","    return np.random.randint(low=0, high=num_machines, size=None)\n","  else:\n","    action = sess.run(chosen_action)\n","  return action\n","\n","\n","with tf.Session() as sess:\n","  sess.run(init)\n","  \n","  for episode in range(num_episodes):\n","    #print('At start of episode', episode, 'total_reward is', total_reward)\n","    \n","    action = select_action(sess, eps)\n","    #print('action:', action)\n","    \n","    reward = get_reward(reward_thresholds[action])\n","    sess.run(update_model, feed_dict={reward_holder: [reward], action_holder: [action]})\n","    \n","    total_reward[action] += reward\n","    num_trials[action] += 1\n","    \n","    if episode % 20 == 0:\n","      print('episode:', episode, 'rewards:', total_reward)\n","print('The actions have been tried for following number of times', num_trials)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["episode: 0 rewards: [-1.  0.  0.]\n","episode: 20 rewards: [-1.  0. 16.]\n","episode: 40 rewards: [-4.  0. 33.]\n","episode: 60 rewards: [-4.  0. 49.]\n","episode: 80 rewards: [-4.  0. 69.]\n","episode: 100 rewards: [-5.  0. 86.]\n","episode: 120 rewards: [ -5.  -1. 105.]\n","episode: 140 rewards: [ -6.  -2. 121.]\n","episode: 160 rewards: [ -6.  -3. 138.]\n","episode: 180 rewards: [ -7.  -3. 151.]\n","episode: 200 rewards: [ -8.  -2. 169.]\n","episode: 220 rewards: [ -8.  -2. 187.]\n","episode: 240 rewards: [ -8.  -2. 207.]\n","episode: 260 rewards: [ -8.  -2. 227.]\n","episode: 280 rewards: [ -9.  -2. 246.]\n","episode: 300 rewards: [ -9.  -2. 266.]\n","episode: 320 rewards: [ -9.  -2. 284.]\n","episode: 340 rewards: [ -9.  -3. 303.]\n","episode: 360 rewards: [-13.  -2. 316.]\n","episode: 380 rewards: [-13.  -2. 334.]\n","episode: 400 rewards: [-14.  -1. 348.]\n","episode: 420 rewards: [-14.  -1. 366.]\n","episode: 440 rewards: [-15.  -2. 384.]\n","episode: 460 rewards: [-16.  -2. 401.]\n","episode: 480 rewards: [-17.  -3. 415.]\n","episode: 500 rewards: [-18.  -3. 430.]\n","episode: 520 rewards: [-18.  -3. 448.]\n","episode: 540 rewards: [-18.  -3. 466.]\n","episode: 560 rewards: [-19.  -3. 485.]\n","episode: 580 rewards: [-19.  -2. 504.]\n","episode: 600 rewards: [-20.  -2. 519.]\n","episode: 620 rewards: [-21.  -2. 538.]\n","episode: 640 rewards: [-22.  -2. 557.]\n","episode: 660 rewards: [-23.  -2. 576.]\n","episode: 680 rewards: [-25.  -2. 594.]\n","episode: 700 rewards: [-26.  -3. 610.]\n","episode: 720 rewards: [-26.  -4. 629.]\n","episode: 740 rewards: [-28.  -3. 646.]\n","episode: 760 rewards: [-30.  -3. 662.]\n","episode: 780 rewards: [-30.  -2. 681.]\n","episode: 800 rewards: [-31.  -4. 698.]\n","episode: 820 rewards: [-31.  -4. 716.]\n","episode: 840 rewards: [-31.  -4. 736.]\n","episode: 860 rewards: [-31.  -4. 754.]\n","episode: 880 rewards: [-31.  -4. 774.]\n","episode: 900 rewards: [-32.  -4. 793.]\n","episode: 920 rewards: [-32.  -4. 813.]\n","episode: 940 rewards: [-32.  -4. 831.]\n","episode: 960 rewards: [-32.  -3. 850.]\n","episode: 980 rewards: [-33.  -3. 865.]\n","The actions have been tried for following number of times [ 34.  39. 927.]\n"],"name":"stdout"}]},{"metadata":{"id":"NCKa_IhCTvwW","colab_type":"code","colab":{}},"cell_type":"code","source":["\n"],"execution_count":0,"outputs":[]}]}